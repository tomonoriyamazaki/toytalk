
import requests
import base64
import json
import subprocess
import time
import webrtcvad
import pyaudio
import wave
import collections

API_ENDPOINT = "https://ujh8l09at7.execute-api.ap-northeast-1.amazonaws.com/dev/raspi"
AUDIO_INPUT = "output.mp3"
AUDIO_OUTPUT = "response.aac"
HISTORY = []

def vad_record_to_mp3(output_path="output.mp3", aggressiveness=2, silence_duration=0.7):
    RATE = 48000
    CHANNELS = 1
    FORMAT = pyaudio.paInt16
    FRAME_DURATION = 30  # ms
    FRAME_SIZE = int(RATE * FRAME_DURATION / 1000)
    VAD = webrtcvad.Vad(aggressiveness)

    p = pyaudio.PyAudio()
    stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE,
                    input=True, frames_per_buffer=FRAME_SIZE)

    ring_buffer = collections.deque(maxlen=int(1000 * silence_duration / FRAME_DURATION))
    triggered = False
    voiced_frames = []

    print("ğŸ¤ VADéŒ²éŸ³ã‚’é–‹å§‹ã—ã¾ã™ã€‚å–‹ã£ã¦ãã ã•ã„â€¦")

    while True:
        frame = stream.read(FRAME_SIZE, exception_on_overflow=False)
        is_speech = VAD.is_speech(frame, RATE)

        if not triggered:
            ring_buffer.append((frame, is_speech))
            num_voiced = len([f for f, speech in ring_buffer if speech])
            if num_voiced > 0.9 * ring_buffer.maxlen:
                triggered = True
                voiced_frames.extend([f for f, s in ring_buffer])
                ring_buffer.clear()
                print("ğŸ™ï¸ éŒ²éŸ³é–‹å§‹")
        else:
            voiced_frames.append(frame)
            ring_buffer.append((frame, is_speech))
            num_silence = len([f for f, speech in ring_buffer if not speech])
            if num_silence > 0.9 * ring_buffer.maxlen:
                print("â¹ï¸ éŒ²éŸ³çµ‚äº†")
                break

    stream.stop_stream()
    stream.close()
    p.terminate()

    temp_wav = "temp.wav"
    wf = wave.open(temp_wav, 'wb')
    wf.setnchannels(CHANNELS)
    wf.setsampwidth(p.get_sample_size(FORMAT))
    wf.setframerate(RATE)
    wf.writeframes(b''.join(voiced_frames))
    wf.close()

    subprocess.run([
        "ffmpeg", "-y",
        "-i", temp_wav,
        "-filter:a", "volume=7",
        output_path
    ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

while True:
    print("\nğŸ¤ ç™ºè©±ã—ã¦ãã ã•ã„ï¼ˆVADã§è‡ªå‹•æ¤œå‡ºï¼‰...")

    # 1. éŒ²éŸ³ï¼ˆVADã§å¯å¤‰é•·ï¼‰
    vad_record_to_mp3(AUDIO_INPUT)

    # 2. mp3â†’base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    with open(AUDIO_INPUT, "rb") as f:
        encoded = base64.b64encode(f.read()).decode("utf-8")

    # 3. POSTï¼ˆå±¥æ­´ä»˜ãï¼‰
    payload = {
        "audio": encoded,
        "history": HISTORY
    }

    try:
        response = requests.post(
            API_ENDPOINT,
            headers={"Content-Type": "application/json"},
            json=payload
        )
        data = response.json()
    except Exception as e:
        print("âŒ é€šä¿¡ã‚¨ãƒ©ãƒ¼:", e)
        continue

    # 4. éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¦å†ç”Ÿ
    with open(AUDIO_OUTPUT, "wb") as f:
        f.write(base64.b64decode(data["voice"]))

    # 5. ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›
    user_text = data.get("user_text", "")
    assistant_reply = data.get("response_text", "")
    timing = data.get("time", {})

    print(f"\nğŸ§‘â€ğŸ¦± {user_text}")
    print(f"ğŸ¤– {assistant_reply}")

    if timing:
        print("\nâ±ï¸ å‡¦ç†æ™‚é–“:")
        print(f"  - STT    : {timing.get('stt', '?')} ç§’")
        print(f"  - LLM    : {timing.get('llm', '?')} ç§’")
        print(f"  - TTS    : {timing.get('tts', '?')} ç§’")
        print(f"  - TOTAL  : {timing.get('total', '?')} ç§’")

    subprocess.run(["ffplay", "-autoexit", AUDIO_OUTPUT], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

    if user_text:
        HISTORY.append({"role": "user", "content": user_text})
    if assistant_reply:
        HISTORY.append({"role": "assistant", "content": assistant_reply})

    time.sleep(0.5)
